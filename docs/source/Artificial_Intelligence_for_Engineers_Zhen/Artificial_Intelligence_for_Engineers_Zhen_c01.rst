Artificial_Intelligence_for_Engineers_Zhen_c01
==============================================

This book presents basic knowledge and essential toolsets needed for people who want to step into artificial intelligence (AI). The 
book is especially suitable for those college students, graduate students, instructors, and IT hobbyists who have an engineering 
mindset. That is, it serves the idea of getting the job done quickly and neatly with an adequate understanding of why and how. It is 
designed to allow one to obtain big pictures for both AI and essential AI topics within the shortest amount of time. Based on the 
picture(s), suitable amounts of theoretical knowledge are contextualized to help the learner gain information about the most 
essential concepts and algorithms. These algorithms are introduced and formulated in a way that the learner can easily implement them 
for real-world applications with a small amount of effort. In short, you read it, you understand it, you try it, and you can solve 
it.

Este libro presenta conocimientos básicos y conjuntos de herramientas esenciales necesarios para las personas que desean entrar en 
inteligencia artificial (IA). El libro es especialmente adecuado para esos estudiantes universitarios, estudiantes de posgrado, 
instructores y aficionados de TI que tienen una mentalidad de ingeniería. Es decir, sirve la idea de hacer el trabajo de manera 
rápida y clara con una comprensión adecuada de por qué y cómo. Está diseñado para permitir que uno obtenga grandes imágenes para 
temas de IA y AI esenciales dentro del menor tiempo. Según las imágenes, se contextualizan cantidades adecuadas de conocimiento 
teórico para ayudar al alumno a obtener información sobre los conceptos y algoritmos más esenciales. Estos algoritmos se introducen y 
formulan de manera que el alumno pueda implementarlos fácilmente para aplicaciones del mundo real con una pequeña cantidad de 
esfuerzo. En resumen, lo lees, lo entiendes, lo intentas y puedes resolverlo.

The book is prepared by a learner for learners. Available learning materials like textbooks are diverse due to the broad scope and 
fast development of AI. There are inconsistent terminologies, distinct focuses of people from different application areas, 
inaccurate/misleading information from the Internet, and fast evolvement of available tools. As a result, a major problem of learners 
is not the lack of information but the opposite: there is too much information. A common frustration is the numerous gaps and 
conflicts that we can easily run into when we try to organize such knowledge to form a consistent and efficient literacy. We may need 
to read multiple books, take several actual or online courses, go through a lot of blogs or tutorials, and learn several tools via 
their user guides/manuals before we finally get what we want. Eventually, we may sadly find that much of the effort is unnecessary, 
while a lot of time is used to address the inconsistencies, gaps, and confusion arising during the above process. This book is 
prepared by a learner who has suffered from all the above issues for other learners who want to avoid such issues.

El libro fue preparado por un estudiante para estudiantes. Los materiales de aprendizaje disponibles, como los libros de texto, son 
diversos debido al amplio alcance y al rápido desarrollo de la IA. Existen terminologías inconsistentes, enfoques distintos de 
personas en diferentes áreas de aplicación, información inexacta o engañosa proveniente de internet y la rápida evolución de las 
herramientas disponibles. Como resultado, un problema importante para los estudiantes no es la falta de información, sino lo 
contrario: el exceso de información. Una frustración común son las numerosas lagunas y conflictos que podemos encontrar fácilmente al 
intentar organizar dicho conocimiento para formar una alfabetización consistente y eficiente. Es posible que necesitemos leer varios 
libros, realizar varios cursos presenciales o en línea, consultar muchos blogs o tutoriales y aprender varias herramientas a través 
de sus guías o manuales de usuario antes de obtener lo que queremos. Con el tiempo, lamentablemente, podemos descubrir que gran parte 
del esfuerzo es innecesario, mientras que se dedica mucho tiempo a abordar las inconsistencias, lagunas y confusiones que surgen 
durante el proceso mencionado. Este libro fue preparado por un estudiante que ha sufrido todos los problemas mencionados 
anteriormente para otros estudiantes que desean evitarlos.

This book, though titled Artificial Intelligence, is mostly devoted to numeric AI represented by machine learning algorithms, which 
predominate the so-called third wave/tide of AI. The most common and useful machine learning topics are selected and introduced. This 
includes introductions to topics that are needed to gain a basic understanding of machine learning, such as linear models, decision 
trees, Bayesian algorithms, and clustering algorithms, as well as more advanced topics like deep learning and reinforcement learning. 
It attempts to cover the essential terms, basic/common algorithms, and useful tools that one may encounter in a typical journey for 
learning and performing contemporary AI.

Este libro, aunque titulado Inteligencia Artificial, se centra principalmente en la IA numérica, representada por algoritmos de 
aprendizaje automático, que predominan en la denominada tercera ola/marea de la IA. Se seleccionan y presentan los temas de 
aprendizaje automático más comunes y útiles. Esto incluye introducciones a temas necesarios para adquirir una comprensión básica del 
aprendizaje automático, como modelos lineales, árboles de decisión, algoritmos bayesianos y algoritmos de agrupamiento, así como 
temas más avanzados como el aprendizaje profundo y el aprendizaje por refuerzo. Intenta cubrir los términos esenciales, los 
algoritmos básicos/comunes y las herramientas útiles que uno puede encontrar en un proceso típico de aprendizaje y ejecución de la IA 
contemporánea.

The book aims to strike a balance between being pragmatic and theoretical. Many AI learners, especially those in engineering 
applications, tend to solve a problem as quickly as possible, for example, using some AI code or libraries from the Internet. This 
usually works well considering the maturity of many AI tools. However, it may lead to unnecessary, inappropriate use of such tools 
and hinder further learning of the topics. On the contrary, some other learners try to start with intricate math and bottom-level 
computer science knowledge, which can easily discourage them and eventually turn out to be not needed in many cases. Each of the 
topics (or chapters) in this book adopts its own storyline, which may share a commonality with other topics while still maintaining 
uniqueness that stems from its own historical development and algorithmic nature.

El libro busca un equilibrio entre pragmatismo y teoría. Muchos estudiantes de IA, especialmente aquellos en aplicaciones de 
ingeniería, tienden a resolver un problema lo más rápido posible, por ejemplo, utilizando código de IA o bibliotecas de internet. 
Esto suele funcionar bien considerando la madurez de muchas herramientas de IA. Sin embargo, puede llevar a un uso innecesario e 
inapropiado de dichas herramientas y dificultar el aprendizaje posterior de los temas. Por el contrario, otros estudiantes intentan 
comenzar con matemáticas complejas y conocimientos básicos de informática, lo que puede desanimarlos fácilmente y, con el tiempo, 
resultar innecesario en muchos casos. Cada uno de los temas (o capítulos) de este libro adopta su propia narrativa, que puede 
compartir puntos en común con otros temas, a la vez que mantiene la singularidad derivada de su propio desarrollo histórico y 
naturaleza algorítmica.

The book is designed for a typical undergraduate, graduate, or dual-listed course with a semester-based calendar. Accordingly, its 16 
chapters present 16 essential topics for the 15 to 16 weeks in a typical university semester. The book has been tested in a 
dual-listed course containing both undergraduate and graduate students from engineering, in which each week had one lecture and one 
computer lab for each topic. Thus, additional teaching and learning materials, including lecture videos, notes, and code, are 
available. The book and associated materials have been proven to be able to assist a student with limited or even no knowledge in 
coding, data analytics, and machine learning to develop a basic and competing ability to understand, communicate, and implement AI. 
Due to the same reason, the book can also be considered by learners who are not an expert on or major in computer science but want to 
take 15 to 16 weeks or 45 cumulative hours (1.5 hours per session × 2 sessions per week × 15 weeks) to learn AI. This book can also 
be used as a reference book for more advanced AI practitioners who want to check the basic algorithms for reviewing a learned 
algorithm or learning new unfamiliar topics.

El libro está diseñado para un curso típico de pregrado, posgrado o de doble titulación con un calendario semestral. Por lo tanto, 
sus 16 capítulos presentan 16 temas esenciales para las 15 a 16 semanas de un semestre universitario típico. El libro se ha probado 
en un curso de doble titulación con estudiantes de pregrado y posgrado de ingeniería, donde cada semana se impartía una clase 
magistral y un laboratorio de informática para cada tema. Por lo tanto, se dispone de materiales adicionales de enseñanza y 
aprendizaje, como vídeos de clases, apuntes y código. Se ha demostrado que el libro y los materiales asociados pueden ayudar a 
estudiantes con conocimientos limitados o nulos de programación, análisis de datos y aprendizaje automático a desarrollar una 
capacidad básica y competitiva para comprender, comunicar e implementar la IA. Por la misma razón, el libro también puede ser 
considerado por estudiantes que no son expertos ni se especializan en informática, pero que desean dedicar de 15 a 16 semanas o 45 
horas acumuladas (1,5 horas por sesión × 2 sesiones por semana × 15 semanas) a aprender IA. Este libro también se puede utilizar como 
libro de referencia para profesionales de IA más avanzados que quieran comprobar los algoritmos básicos para revisar un algoritmo 
aprendido o aprender nuevos temas desconocidos.

This book starts with a chapter for the basics of AI, which both outlines a big AI picture by discussing why to look into AI, what is 
AI, the history of AI, AI vs. traditional engineering methods, as well as AI applications in engineering and other sectors, and 
presents the AI basics including the basic concepts, common algorithms, and challenges and issues. Next, in Chap. 2, common tools 
needed for implementing AI, including coding language and environment, data manipulation and visualization tools, machine learning 
and data analysis libraries, and deep learning packages, will be introduced. Following that, typical supervised learning topics, 
including linear models, decision trees, support vector machines, Bayesian algorithms, neural networks, and deep learning, will be 
presented in Chaps. 3–7. Ensemble learning will also be introduced in Chap. 9 based on them. After that, four typical unsupervised 
learning topics, i.e., clustering, dimensionality reduction, anomaly detection, and association rule learning, will be explored in 
Chaps. 10–13. Reinforcement learning will be introduced using two chapters: Chap. 14 for the basics of reinforcement learning and 
value-based reinforcement learning, and Chap. 15 for policy-based reinforcement learning. Finally, Appendix A delivers appendices for 
more detailed information on AI’s foundational and ancillary knowledge in math, optimization (solvers), and evaluation metrics.

Este libro comienza con un capítulo sobre los fundamentos de la IA, que describe un panorama general de la IA al explicar por qué 
considerarla, qué es, su historia, la IA frente a los métodos de ingeniería tradicionales, así como las aplicaciones de la IA en la 
ingeniería y otros sectores. Además, presenta los fundamentos de la IA, incluyendo conceptos básicos, algoritmos comunes, desafíos y 
problemas. A continuación, en el capítulo 2, se presentarán las herramientas comunes necesarias para implementar la IA, incluyendo 
lenguajes y entornos de codificación, herramientas de manipulación y visualización de datos, bibliotecas de aprendizaje automático y 
análisis de datos, y paquetes de aprendizaje profundo. Posteriormente, en los capítulos 3 a 7, se presentarán temas típicos de 
aprendizaje supervisado, incluyendo modelos lineales, árboles de decisión, máquinas de vectores de soporte, algoritmos bayesianos, 
redes neuronales y aprendizaje profundo. El aprendizaje por conjuntos también se presentará en el capítulo 9, basándose en estos 
temas. Posteriormente, en los capítulos 10 a 13, se explorarán cuatro temas típicos de aprendizaje no supervisado: agrupamiento, 
reducción de dimensionalidad, detección de anomalías y aprendizaje de reglas de asociación. El aprendizaje por refuerzo se presentará 
en dos capítulos: el capítulo 14, que aborda los fundamentos del aprendizaje por refuerzo y el aprendizaje por refuerzo basado en 
valores, y el capítulo 15, que aborda el aprendizaje por refuerzo basado en políticas. Finalmente, el Apéndice A ofrece apéndices con 
información más detallada sobre los conocimientos fundamentales y complementarios de la IA en matemáticas, optimización 
(solucionadores) y métricas de evaluación.


Chapter 1
Preparation Knowledge: Basics of AI

1.1 Overview

This chapter begins with an introduction to artificial intelligence (AI) to discuss the first things that most AI learners want to 
know: why look into AI, what is AI, the history of AI, AI versus traditional engineering methods, and AI applications in engineering 
and other sectors. Next, the basics of AI are laid out, including the basic concepts, common algorithms, and challenges and issues.

Mathematics takes different roles in different AI topics and is thus essential in the understanding and implementation of many of 
them. Considering the possibly diverse backgrounds of the readers, such knowledge is provided systematically at the end of the book 
as Appendices. Mathematics knowledge that is needed for AI, i.e., statistics, information theory, and array operations, can be 
reviewed there as needed. Some math knowledge that is essential to specific AI topics will be provided in the corresponding chapters.

1.2 Introduction to Artificial Intelligence
1.2.1 Why Look into AI?

Deemed the core of the fourth industrial revolution or Industry 4.0, AI has been reshaping our lives in many ways towards what we 
have long pictured in science fiction. From chatbots like ChatGPT to autonomous cars, more widespread AI applications have been 
transforming our society, bringing benefits such as increased efficiencies, more intelligent products, and fewer repetitive tasks. AI 
is projected to boost the corporate profitability of 16 industries in 12 economies by an average of 38% by 2035 [1].


The popularity and impacts of AI can be seen in related intellectual products. First, the number of AI-related publications almost 
tripled in one decade, e.g., from 88,000 in 2010 to 240,000 in 2022, in AI categories like pattern recognition, machine learning, and 
algorithms [2, 3]. Besides publications, the number of AI-related patent filings multiplied by a factor of 30 times from 2015 to 
2021. The global market size of AI software is expected to grow at a rate of 22.97% from 2023 and surpass $1094.52 billion by 2032 
[4].

This so-called third wave/tide of AI has enabled world-changing applications represented by deep learning and reinforcement learning. 
Deep learning gained success in application areas like computer vision and natural language processing. Computer vision is an AI 
subfield that teaches machines to understand images and videos for the purposes of image classification, object detection, mapping of 
position and movement of objects, and so on. In addition to understanding existing images, AI can also generate “fake” images and 
videos that are nearly indistinguishable from real ones. Natural language processing, a subfield that focuses on summarizing 
contents, inferring outcomes, identifying emotional context, speech recognition and transcription, and translation, is impacting our 
lives again via many language-related innovations and tools, attributed to its cumulative development since the 1950s and recent 
breakthroughs via deep learning especially large language models. Besides deep learning, reinforcement learning is another major area 
of AI innovations, which helps us advance the cutting edge of AI. Its triumphs have swept from video games to complicated board games 
like chess and Go and, more recently, in engineering decision-making and control tasks like robotics.

AI grows and impacts the world via strong synergy with the blooming of data and improvements in computing hardware. The third wave of 
AI is propelled by big data, improvements in computing software (GPU), and advances in machine learning algorithms. In engineering, 
the explosion of data is partly attributed to the widespread use of low-cost information-sensing mobile devices, remote sensing, 
software logs, cameras, microphones, radio-frequency identification (RFID) readers, and wireless sensor networks. The vast amounts of 
data are enabling us to explore AI tools with more complicated architectures, higher capacity, and better generalization, such as 
neural networks that are deeper, wider, and more intricate inside. In return, AI breakthroughs driven by data promoted tech giants 
like Google, Microsoft, Meta, IBM, and Amazon to improve AI tools, algorithms, applications, and data. Owning to the development of 
AI, a system that would have taken 6 minutes to train in 2018 would only need about 13 seconds in 2021 [2]. In terms of cost, the 
costs of AI training drop at a rate of 70% per year as of 2024 [5], outpacing the famous Moore’s Law.

The dramatic development of AI has also been obviously affecting the future of the workforce. More AI applications across industries 
create higher demands for AI education and jobs. Taking the United States, for example, California, Texas, New York, and Virginia 
have exhibited high demands for AI-related occupations. In computer science, the most popular specialties among PhD students have 
been secured by AI and machine learning in the past decade. In addition to the trends in the workforce demand and development, 
corporate investments in AI are at an all-time high, totaling $189 billion in 2023, representing a 1300% increase from 2013, and 
could amount to $200 billion globally by 2025 [3]. AI companies in the “healthcare” sector led the champion, followed by “data 
management, processing, and cloud,” and financial technology (or called “fintech” for short).

AI also generates far-reaching impacts on the economy, politics, mobility, healthcare, security, and the environment. Such influence 
on the economy can take place via disruptions to the labor market, alternation of the nature of long-established roles, and changes 
in political thinking and opinions. For mobility, AI is estimated to be capable of helping reduce the number of road accidents by as 
much as 90% and boosting multimodal transportation through better transportation options and operations, while also bringing about 
new challenges in liability, ethics, and management [6]. The further development of AI for healthcare can possibly eradicate many 
incurable diseases and help deliver care to remote areas and groups with difficulties. In security and defense sectors, AI-powered 
software is dramatically altering the digital security threat landscape via innovative cyberattack detection, prevention, and risk 
control, which could easily save economic loss in excess of $50 billion in one major global cyberattack [7].

Therefore, AI is a must-know for the new generations. For engineers, we may need to know the basics as engineering is being further 
impacted by and fused with AI. Some engineers who will be more exposed to or deal with AI may need to know the common AI techniques, 
from the entry level of having common and useful AI techniques in their toolbox to a more advanced level of assessing, modifying, 
extending, and coding some newer and complicated AI algorithms. This book is proposed to help engineers quickly bridge these gaps.

1.2.2 What Is AI?

AI has been defined from different perspectives by people from many distinct areas and thus encompasses a wide variety of techniques. 
In this book, AI is defined as a method or the use of such a method for making a computer or a computer-controlled agent, either 
hardware like a robot or software like a computer program, to think intelligently like the human mind. Thus, AI is accomplished by 
studying the working mechanisms of human brains and by analyzing the cognitive processes. Such AI studies develop products like data, 
algorithms, intelligent software and systems, and paradigms for specific applications. All of such AI products enable the computer or 
any agent controlled by it to exhibit some types of human beings’ intelligence to some extent.

AI can be classified in different ways, which is a topic that can trigger many inconsistencies, conflicts, and debates. This fact is 
attributed to numerous reasons. In particular, as an area contributed by people from different disciplines, the history, convention, 
and backgrounds of these contributors in AI could lead to the adoption of the same terms for different meanings, names, and usages 
and different terms with the same meaning. In addition, the evolution of the AI field including the technical development and other 
incidents such as rebranding of AI topics for promoting publications and fundraising further created gaps, overlaps, conflicts, and 
confusion when talking about the types of AI as well as the categorization of its subareas.

One classification constructed based on the consensuses in the literature will be adopted throughout this book for consistency. As 
shown in Fig. 1.1, AI can be roughly categorized into general AI and narrow AI. General AI is what we see in science fiction, in 
which AI can enable intelligent agents like supermen, e.g., Terminator and Wall-E, for handling much different tasks like combating 
and flying. Though general AI is always a dream and has been repeatedly discussed in the history of AI including the recent 
artificial general intelligence, it may still be far from us. By contrast, narrow AI is what we have been mostly working on. Narrow 
AI is set for a lower goal and thus more feasible. More interestingly, the successes in narrow AI in different phases of the AI 
history promoted people to pursue general AI.

Narrow AI can be divided into two groups: symbolic AI and nonsymbolic AI (or called numeric AI less frequently). Symbolic AI is 
represented by logic reasoning that was extensively studied in the early stages of AI and by more successful applications in expert 
systems. Nonsymbolic AI mostly refers to machine learning.

What differentiates these two major AI streams is not the use of symbols or languages that can be easily understood by human beings. 
Although symbolic AI is usually associated with the use of symbols or languages, while nonsymbolic AI is not, there are exceptions. 
Instead, they essentially represent two ways of learning or gaining intelligence/knowledge: deduction and induction. Symbolic AI 
features deduction (or deduction reasoning), in which we make inferences based on widely accepted factors or premises, whereas 
nonsymbolic AI boils down to induction (or induction reasoning), in which we extract general knowledge from observations on specific 
cases. From another perspective, deduction goes from general to special, whereas induction moves from special to general. As a 
result, symbolic AI features the use of reasoning, usually performed with languages that humans could understand, while nonsymbolic 
AI, e.g., machine learning, is characterized by learning from or via data.

This book is mostly devoted to machine learning, which is predominant in contemporary AI studies. This arrangement was made 
considering the fact that many people including AI researchers use AI and machine learning interchangeably these days. Despite this 
fact, it is still worthwhile to mention that symbolic AI might not disappear or go out of date. In fact, in some people’s opinions, 
they have more likely dissolved into our lives. Symbolic AI, which is called “good old-fashioned artificial intelligence (GOFAI),” is 
believed by some other people to be the classical and most successful AI approach till now. AI techniques or efforts along this line 
make computers more “intelligent” by using logic, e.g., mathematically provable logical methods, to manipulate “terms” that were 
specific to the target task. Humans could define “rules” for the concrete manipulation of such “terms” and create rule-based systems. 
Thus, computers and smartphones that can fulfill tasks based on predefined rules built on reasoning like “if statements” can also be 
viewed as symbolic AI, though we no longer view them as AI in most of the modern AI contexts.

In machine learning, it is common to classify algorithms into supervised learning and unsupervised learning. In addition, 
semi-supervised learning, as something between and still distinctly different from supervised and unsupervised learning, has also 
caught lots of attention, especially in recent years. Reinforcement learning is usually deemed as another major category of machine 
learning in addition to supervised, unsupervised, and semi-supervised learning. This viewpoint is made based on the thought that 
supervised learning requires a labeled dataset for training, and unsupervised learning identifies hidden data patterns from an 
unlabeled dataset, while reinforcement learning does not require data as it learns by interacting with the environment—generating 
data in the learning process. Notwithstanding, reinforcement learning can be understood using the frameworks of supervised and 
unsupervised learning in some way, which makes some people view it as a special case of supervised or unsupervised learning, 
depending on how labels are defined in the consideration of the rewards. This book primarily covers supervised learning, unsupervised 
learning, and reinforcement learning.

Another way of classifying machine learning, which is based on the major characteristics or underlying mechanisms, groups machine 
learning algorithms into symbolism, connectionism, Bayesianism, evolutionism, and analogism. This classification method is much less 
common. However, some terms from it such as connectionism may frequently appear in the context of AI and may confuse people without 
prior knowledge. Symbolism (investigated by symbolists) here includes symbolic AI in a broad sense. But within machine learning, the 
narrow definition of it refers to methods characterized by the use of symbols and some types of logic reasoning processes together 
with learning from data. Examples include decision trees and random forests. Connectionists or neuroscientists create models based on 
the brain and thus employ artificial neural networks and their variations including the most recent deep learning, which is a rebrand 
of the “machine learning with deep neural networks.” Bayesians, or more broadly as Bayesian methods, treat machine learning as a form 
of probabilistic inference. Examples of this category include naive Bayesian, Bayesian network, hidden Markov chains, and graphical 
models. 

Evolutionaries or biologists use genetic algorithms and evolutionary programming, respectively. It is worthwhile to mention that 
statistical (machine) learning usually represents a much broader concept, in which statistics is used to reinterpret the most popular 
machine learning algorithms. Thus, it overlaps with machine learning and is distinct from the above Bayesian methods (as a 
subcategory of machine learning). The use of genetic algorithms appears to be more frequently discussed in the area of optimization 
than in AI. Analogizers or psychologists fulfill machine learning tasks based on the similarity between samples or groups. Machine 
learning models that analogizers frequently use are K-nearest neighbor algorithms, SVMs, and unsupervised machine learning methods.

1.2.3 History of AI

The history of AI is primarily about technical innovations. However, an in-depth understanding of this history also needs the 
consideration of many financial, political, and cultural factors.

Spawning (1930–1952)

As marked in Fig. 1.2, the fertilization of AI started in myths, stories, and rumors in which artificial beings were endowed with 
consciousness or intelligence by master craftsmen. Next, the implantation occurred with further developments in science fiction, 
e.g., during the Golden Age of science fiction between 1938 and 1946 [8].


Then, AI’s fast maturation into a fetus was driven and marked by breakthroughs like philosophers’ effort at describing the process of 
human thinking and materialized as mechanical manipulation of symbols was realized in the invention of programmable digital computers 
in the 1940s.

During this time, the confluence of several closely related ideas from different areas provided theoretical support for constructing 
the electronic brain, which cast the foundation for modern AI. These include research in neurology that showed the brain is a network 
of neurons fired in all-or-nothing electrical pulses. In particular, Walter Pitts and Warren McCulloch reported the use of networks 
of idealized artificial neurons for performing simple logical functions in 1943 [9], which opened the door to artificial neural 
networks as well as the ups and downs of connectionism in the later AI history. In 1950, Alan Turing made the first serious proposal 
in the philosophy of AI by presenting his famous Turing test, in which a machine is thought to be “thinking” if it could conduct a 
conversation that was indistinguishable from a conversation with a human being [10]. This study allowed Turing to convincingly argue 
that a “thinking machine” was plausible and answered the most common objections to the proposition.

Other breakthroughs included the first stored program computer in 1948, i.e., the Ferranti Mark 1 machine [11], the use of this 
machine to write a checkers program, and Arthur Samuel’s checkers program developed in the mid-1950s and early 1960s, which gained 
skills comparable to respectable amateur players [12]. When access to digital computers became possible in the mid-1950s, a few 
scientists identified a new approach to creating thinking machines: a machine that manipulates numbers could also manipulate symbols, 
and such a manipulation of symbols could be the essence of human thoughts. As a result, the first AI program, i.e., Logic Theorists 
[13], was created in 1955.

Birth (1952 and 1956)

Between 1952 and 1956, the advent of computers inspired a handful of scientists to seriously discuss the possibility of building an 
electronic brain. Usually, it is believed that the milestone where AI becomes a field of study (or an academic discipline) was marked 
by a workshop held on the campus of Dartmouth College, USA, in 1956 [14]. Most event attendees later led AI research with millions of 
dollars of financial support, and many of them predicted machines as intelligent as human beings would be created in no more than one 
generation.

Symbolic AI (1956–1974)

After the Dartmouth Workshop, fast developments in AI programs, especially symbolic AI, were achieved for applications such as 
solving algebra problems, proving theorems in geometry, and learning to speak English. There were many efforts at maze-alike games in 
the paradigm of “reasoning as search.” Attempts at enabling computers to communicate in natural languages yielded programs like 
“Student,” AI programs written using a semantic net (conceptual dependency theory), and chatterbots (later clipped to chatbots) like 
ELIZA [15, 16]. The MIT AI Laboratory proposed to focus on artificially simple situations known as micro-worlds based on a perception 
that, in successful sciences like physics, basic principles were often best understood using simplified models like frictionless 
planes or perfectly rigid bodies. In Japan, the world’s first full-scale “intelligent” humanoid robot or android was created via the 
WABOT (WAseda roBOT 1) project.

Such successes led to overoptimism among the first generation of AI researchers as well as some funding agencies. Many researchers 
believed that, within ten to twenty years, the problem of creating AI and generating machines that can handle most of human beings’ 
work would become possible. Meanwhile, the optimism prompted major research funding, such as that from the Advanced Research Projects 
Agency in the United States (later known as DARPA).

First AI Winter (1974–1980)

In the 1970s, AI started receiving major critiques and financial setbacks. In particular, AI researchers failed to appreciate the 
difficulty of the AI problems they faced. Meanwhile, the overoptimism had raised expectations impossibly high, and when the promised 
results failed to be delivered, funding for AI was withdrawn. During this time, the field of connectionism (or neural nets) was shut 
down almost completely for 10 years partially due to Marvin Minsky’s devastating criticism of perceptrons [17]. Despite the recession 
and criticisms against AI in the late 1970s, new ideas were explored in logic programming, commonsense reasoning, and many other 
areas.

Expert System and Connectionism Bloom (1980–1987)

In the 1980s, a form of AI program called “expert systems” gained popularity in the industry worldwide [18], and knowledge became the 
focus of mainstream AI research. In the same period, the government of Japan aggressively funded AI through its fifth-generation 
computer project [19]. Another remarkable advance in the early 1980s was the revival of connectionism represented by the work of John 
Hopfield and David Rumelhart [20]. Once again, AI gained success in a variety of ways during this relatively short blooming period.

Second AI Winter (1987–1993)

The rise and drop of AI in the 1980s, especially the involvement of industries and governments, exhibited a clear correlation with 
the economy. The AI collapse in the later 1980s was partially due to the failure of commercial vendors to develop a wide variety of 
workable solutions and the burst of the economic bubble during that time. The AI technology was deemed not viable as the public was 
also discouraged by the failures of dozens of companies. Despite the setbacks and pessimism, the AI field continued to advance in 
multiple ways. For example, many researchers, including robotics developers Rodney Brooks and Hans Moravec [21], advocated 
approaching AI in other ways.

Recovery (1993–2011)

After more than half a century of development, the field of AI finally achieved
some of its oldest goals. In particular, widespread use of many AI techniques in
industries finally became realistic. Some of the successes were due to the increasing
computer power, while some others were achieved by focusing on specific problems
and pursuing them with the highest standards of scientific accountability. Despite
the progress, the reputation of AI, at least in the business world, was less than
pristine. Within the field, there was little agreement on the reasons why AI failed
to fulfill the dream of human-level intelligence in the 1960s. All these factors led
to the evolvement of AI into competing subfields focused on particular problems or
approaches, sometimes even under new names to blur, rebrand, or dissociate their
AI tattoo. This is a time when AI became both more cautious and more successful
than it had ever been.

Deep Learning and Big Data Rise (2011–present)
The third wave marked the explosive development of deep learning, which may be
attributed to three factors: improvements in deep learning architectures, especially
for addressing the vanishing gradient issue, increases in computational power
represented by the use of GPU, and growth of data in a “big data” era especially
image data. Important milestones in these three aspects include the publication
of the deep belief networks [22], the advocacy for the use of GPUs for training
deep neural networks [23], the launch of the ImageNet database (14 million labeled
images) that led to later ImageNet annual competitions (ILSVRC) [24], and the
development of ReLU and other techniques for the vanishing gradient problem [25].
Then in 2012, AlexNet’s victory in the ImageNet competition triggered a new deep
learning bloom globally and attracted industry giants’ attention [26]. Deep learning
started gaining more momentum and making impacts in or even sweeping many
disciplines such as computer vision and natural language processing. Advancements
in algorithms such as generative adversarial network (GAN) [27] and the continuous
development of deep learning architectures for various purposes, platforms, and

tasks have been continuing. This led to the dramatic growth of the market for AI-
related products, which was called an AI “freeze” by the New York Times. During

this time, the rise of reinforcement learning, especially its integration with deep
learning, also generated astonishing breakthroughs, and further developments along


this direction gained more momentum and popularity. This freeze continues as large
language models and generative AI impact and reshape many business areas [28].

1.2.4 AI Versus Traditional Engineering Methods
From the mathematical perspective, most engineering problems boil down to the
problem of finding a mapping, f , from the input, x, to the output, y:

y = f (x) (1.1)
Traditional physics-based engineering methods and data-driven methods like
numerical AI (machine learning) are just two different ways of finding such a
mapping. Coupled physics-based analysis represented by multiphysics, multiscale,
and multi-fidelity and AI analysis represented by deep and reinforcement learning
reflect the trends of collaboration and automation in modern industries, respectively,
and both address the needs for more complicated and accurate analysis. Thus, both
types of methods represent the future directions of modern industries and deserve
close attention regardless of the rises and falls of AI compared with traditional

engineering analysis. Traditional physics-based methods use mathematical equa-
tions derived on the basis of physics rules to construct models. These models are

employed to explain or predict phenomena, e.g. states and processes.
By contrast, machine learning aims to find a model that approximates the solution
to a real-world problem by analyzing the data. Such models need to be constructed
with tools that computers can conveniently process and human beings can easily
understand. Thus, mathematical models like basic mathematical functions (like
polynomials) or linear algebra equations (formulated using arrays) are intuitive
choices. In fact, mathematical models are what have been predominantly adopted.
These mathematical functions are also referred to as “mathematical models” or just
models. As a result, machine learning models are usually viewed as mathematical
equations/functions that represent or model real-world problems/scenarios. Terms
such as “mathematical form,” “mathematical being,” and “machine” are also used in
the replace of “models” in some literature. In some cases, machine learning models
are also called function approximations. This is because it is usually difficult to find
exact functions to represent real-world problems.
Therefore, data-driven AI, like machine learning, also uses models. However,
distinct from physics-based methods, machine learning derives the model from data
during the analysis instead of constructing the model based on physics before the
analysis.
As illustrated in Fig. 1.3, in traditional engineering analysis, we first build
the model, e.g., a mathematical model that derives the analytical solution and a
numerical model like a finite element analysis model, for a problem based on its
underlying physics. Then this model, together with some data that informs the


initial/boundary conditions and material properties, will be fed into the model to
obtain an answer, which can be predictions of some state variables or others.
In machine learning analysis, the construction of the model occurs during the
analysis instead of before the analysis. Also, the construction of the model is a
significant or even a major part of machine learning analysis in many cases. The
word “training” is used to refer to this process of generating the model. Due to
this reason, this word is used everywhere in data-driven methods. Notwithstanding,
it may be totally new to people who are not familiar with machine learning.
To generate a model in machine learning, we need to get the basic construction
materials:
(1) Data and some types of predefined knowledge about the data (like labels in
supervised learning, metrics to assess data in unsupervised learning, and reward
functions in reinforcement learning)
(2) A way of constructing the model, or more commonly, called an algorithm
In particular, algorithms dictate how data can be used to generate models.
Therefore, algorithms form a major body of the knowledge for machine learning.
That is, machine learning is usually introduced in terms of different types of
algorithms. By contrast, “models” represent the knowledge extracted from the data
based on the employed algorithms. In a simple way, we can understand data as
cooking materials, algorithms as recipes, and models as cooked food or dishes.
Thus, a typical machine learning cooking book like this book does not talk about
“models” specifically in detail, but instead, it focuses more on common recipes,
typical ingredients, and their treatments, as well as tricks for cooking and evaluating
dishes.
In machine learning, as illustrated in Fig. 1.3, after a model is obtained or
“trained,” this trained model will be used to obtain answers to new problems based
on new data for these problems. This “testing” process is similar to the traditional
engineering analysis.


Practice: Prediction of Object Flying Trajectory (Physics Methods Versus
Data Method)
A simple engineering problem is employed here to show how data-driven methods
and traditional physics-based engineering methods can be used to solve the sample
problem. The problem to be considered is to analyze and predict the trajectory of a
ball. As shown in Fig. 1.4, the ball moves off the edge of an object with a horizontal
velocity of v0 = 5 m/s. The goal is to find out the trajectory of the moving ball from
0 s to 10 s in terms of a function y = f (x) with a gravity constant of g = 9.81 m/s2.
Let us first take a look at two common traditional engineering methods:
analytical solution and numerical analysis. Both methods are performed based on
the physics underlying the process and a mathematical formulation of the process.
The analytical method involves framing the problem in a well-understood form and
calculating the exact solution. Numerical analysis is based on a numerical procedure
that approaches the solution to the problem in a continuous world using a numeric
approximation.
To solve the above problem using the analytical method, we first need to recall
the major physics: Newton’s second law (f = m · a). Along the x (horizontal)
direction, the initial velocity will remain as a constant, because no force along the
horizontal direction implies no acceleration. Thus, the traveling distance is x = v0·t,
in which v0 is the initial velocity of the ball along the horizontal direction and t is
time. Along the y (vertical) direction, m · g = m · a, so the acceleration along the
vertical direction is g. Accordingly, the vertical velocity is g · t and the traveling
distance is y = 1

2 gt2. Based on the above formulations, we can easily derive the

function y = f (x) as follows:
y = 1
2
gt
2 = 1
2
g
 x
v0
2
= g
2v2
0
x2 (1.2)


Based on the above deduction, the analytical solution is obtained by substituting
the known constants into the equation: y = 0.01962x2.
Numerical Analysis is usually preferred for complicated problems, especially
those that cannot be easily addressed with an analytical solution, for example,
problems with high nonlinearity and high dimensionality. Thus, it is usually not
adopted for the above simple problem. Here, we just use it to show the idea of
numerical analysis. For this purpose, we discretize the time, e.g., using a timestep
of 0.1 s for the 10 s. Then, the numerical solution to the above problem can be
obtained via the following iterative process.
Numerical process for predicting the trajectory:
Initialize distance and velocity: xi = 0, vxi = v0, yi = 0, vyi = 0, i = 0, and
t = 0.1
Repeat until i = 100 (i.e., 10/0.1):
xi = vxi · t
vyi ← vyi + t , and yi ← yi + vyi · t
i = i + 1
Data-Driven Methods seek the solution from data. Thus, distinct from the above
physics-based methods, data-driven methods need to obtain some data first. Such
data can be obtained from experiments or computer simulations. To illustrate this
process, let us generate some data from the analytical solution and add random
noise to represent “experimental data” with different sources of errors in real-world
systems and measurements. Then, if we use a second-order polynomial function to
fit the “experimental data,” we can also obtain a model that describes the data. Such
a model can also be used for predicting future behavior, for example, the trajectory
beyond 10 s.
The results of the three methods and the “experimental data” are shown in
Fig. 1.5. In this simple example, we can see that these three methods can help us
achieve the same goal with comparable performance. In real-world engineering
practices, the selection of methods usually relies on the characteristics of the
problem, such as the nature and complexity of the problem, the knowledge about
the physics and material properties, the available computational power, and the
expectation for computing time and accuracy. Such a selection can be complicated
and may require a certain level of expertise.

1.2.5 AI Applications
AI Applications in All Sectors

Aiming at enabling a machine to think itself, AI has been impacting the devel-
opment of many sectors since its pre-birth, especially during the high tides of its


development. In the fourth industrial revolution, AI, automation, and big data are
transforming virtually every sector. In particular, machine learning is being widely
used in approximately all the sectors, including information technology, healthcare,
finance, materials, communication services, etc. Some representative and impactful
real-world examples of machine learning applications are listed below.
Autonomous Cars
Autopilot represented by autonomous cars is one of the most exciting applications of
machine learning in today’s world. The use of various machine learning techniques
with the data from different sensors enabled different levels of vehicle autonomy.
The integration of AI with electric cars has been revolutionizing the automotive
industry and incubating many tech unicorns and new industry leaders in advanced
driver assistance systems (ADAS) and autonomous driving like Tesla and BYD.
The demands for technical innovations in this area also drive the development of
technologies in computer vision, planning, decision-making, and control.
Smart Assistants
Speech recognition and face recognition are among the earliest and most popular

applications of machine learning in the third wave of AI. Nowadays, most smart-
phones, tablets, and desktops provide voice search functions or voice-controlled

personal assistants, which can communicate with the device users by speaking
human languages. The core of such AI tools includes speech recognition as well
as other AI applications for voice recognition, natural language processing, and
personalized recommendations. There are many well-known examples including
Google Assistant, SIRI, Alexa, and Cortana. Together with face recognition, speech
recognition has become the interface for many other AI applications such as smart
home.


Healthcare
AI tools based on machine learning algorithms have gained considerable acceptance
in healthcare industries. For example, deep learning has become a useful tool for
helping specialists analyze external medical data on patients’ conditions such as CT,
MRI, ultrasound, and various screening tests. In addition, machine learning has been
explored as a way to assist disease diagnosis, find hidden knowledge from medical
data, and recommend treatment methods. Other than treatment, machine learning
has also been explored for automatic billing, clinical decision support, development
of clinical care guidelines, and so on.
Recommendations AI
Most of us should have had some experience with AI-powered advertising. For
example, after you perform a search for an item on Amazon, you may soon find that
many web pages and apps that you browse later contain advertisements about this
item. This is a typical example of recommendations AI, which reshaped our lives

silently and significantly. Another example is the use of similar tools in content-
based online social networks like YouTube, Flicky, and TikTok. Such apps can

quickly detect your tastes and preferences and then deliver the most related contents
to you.
Robots
Robots were studied and adopted in industries even before the new wave of AI.
However, the advances in AI have been helping these machines with different
levels of autonomy to acquire higher intelligence. Such intelligence can be gained
via computer vision, natural language processing, decision-making ability via
reinforcement learning, and general machine learning for prediction and diagnosis.
The consequent innovations can bring forth increased uptime (e.g., identify issues
and predict maintenance time), reduced programming time (e.g., less programming),
and higher productivity (e.g., multitask, collaboration with humans and other robots)
to industrial robots.
Finance
In the sector of finance and marketing, machine learning has also made remarkable
progress in reshaping this area. First, the use of marketing chatbots and automated
financial investing has been widely accepted. Besides, machine learning can now
help marketers create various hypotheses, perform tests and evaluations, analyze
financial data, and make predictions for future trends or events. Quantitative finance
has become mainstream as stock trading bots are handling most of the trading
based on calculations from machine learning algorithms. Deep learning models like
convolutional neural network, recurrent neural network, and other deep learning
architectures helped build such trading models.
Traffic Prediction
Transportation is another area where AI has gained good ground for development
and application. In addition to connected and autonomous vehicles (CAVs), the
prediction and management of traffic have also been benefiting from AI. Navigation
tools like Google Maps now rely on machine learning to help us find the shortest


route and predict traffic conditions to make such results in real time. In the

future, such efforts will be further integrated with vehicle-based sensors, vehicle-
to-everything (V2X, including infrastructure), social media, and other data sources

to create more intelligent transportation systems.
AI Applications in Engineering
The applications of AI in the engineering sector is not as widespread as it should be.
Notwithstanding, AI has been widely accepted as a significant future direction and
an essential component of engineering. In particular, it has been widely recognized
that the sooner AI is adopted, the sooner the engineering will reap its benefits. Also,
the sector can stay competitive and sustain its global leadership by embracing the
most advanced AI applications. Though possibly still in the early stage, AI is now
reconstructing the ways of engineering practices. The following is a summary of
some typical influential examples of AI applications in engineering. Engineers need
to prepare themselves to be literate in the widespread AI tools and learn to work in
collaboration with software and machines equipped with AI.
Data Processing
Autonomous processing and analysis of engineering data may be one of the first
benefits that AI brings to engineers. This is because engineering is on the front line
of data generation and utilization due to the ubiquitous use of sensing and testing
equipment and the extensive adoption of data for various monitoring, analysis,
design, and management tasks. Examples of such data include sensor readings,
drawings, documents, 3D models, measurements, simulation results, and image and
video data. AI applications ranging from conventional machine learning methods
to more advanced deep learning approaches for image processing and text/visual
recognition can all assist or replace engineers in processing and analyzing such data
even in their raw formats. This can free engineers from many labor-intensive data
processing and analysis work for improved productivity, objectivity, and accuracy.
System Monitoring
AI can be applied to monitor the performance of many engineering systems for
various benefits, such as determining the time for maintenance, identifying system
errors as anomalies, and recommending system operations like downtime selection.
Such efforts have been attempted in a variety of engineering systems such as energy,

civil infrastructure, and mechanical systems. For example, agencies like the Electri-
cal Power Research Institute (EPRI) and the US Department of Transportation have

explored the way of using machine learning algorithms to analyze images taken by
drones to detect malfunctions and distress in the infrastructure.
Prediction
Many engineering tasks involve the prediction of time- or event-dependent factors.
Typical examples include the service life of structural and mechanical components,
the cost and time needed for specific engineering jobs like construction, the traffic


energy use in regular scenarios and special events, the behavior of many systems
under external loadings, and so on. Many of such tasks may suffer from the lack of
information and the existence of high uncertainty due to unknown factors. Under
such conditions, traditional engineering methods may be hard to apply or yield
results with needed accuracy and timeliness. Fortunately, these are where AI tools
can step in and prevail.
Design
Design is a core practice in many engineering disciplines. Taking civil engineering
for example, CAD (computer-aided design) has been widely adopted to help
designers propose, visualize, compare, evaluate, and communicate designs. Now
at the cutting edge, 3D CAD has been integrated with the building information
modeling (BIM) to generate a tool more than just a 3D model to design building
systems. With the assistance of BIM, engineers can design multidimensional models
of the projects in a simulation before their execution in the field. Also, AI tools can
bring autonomous data synthesis and analysis, estimates of parameters, time and
costs, and decision-making for selecting methods and materials to the BIM. This
will possibly further engineering design to a more efficient, powerful, automated,
and accurate practice.
Management
The management of human resources, facilities and equipment, data and other
digital assets, monitoring and testing programs, and materials is sometimes the
major part of many engineering applications and determines the efficiency, cost, and
outcomes of these applications. Many engineering management practices still rely
on subjective and manual operations. Taking asset management in transportation
agencies, for example, the prevalent management practices still heavily depend on
engineers’ presence, documentation, and judgment. These are exactly where AI can
easily outperform human beings. Therefore, AI applications have great potential in
management.
Optimization
AI is heavily coupled with optimization because AI is supposed to help us get the
best mapping from the input to output. This fact may bring forth one benefit: AI
can easily explore a great amount of existing data or possible conditions to obtain
the possibility that best suits our needs. For discovery, AI has been studied for
its potential of identifying better materials, manufacturing/construction procedures,
testing schemes, and system operation approaches. AI also has the potential of
serving as a higher-level decision-making tool to guide the maneuver of other tools,
such as the use of reinforcement learning to guide basic controllers in robotics and
other automation applications.
Automation
AI has been a popular option for implementing cyber-physical systems. This is

because AI can enable many systems to handle complex tasks in addition to self-
management and self-healing. In addition, AI helps many systems gain the ability to

learn and upgrade themselves. Due to these reasons, many tasks that were performed


by human engineers can now be carried out by robots. For example, the use of
advanced robots in automobile manufacturing has been steadily increasing. AI has
been explored to serve multiple roles in automation from being in charge of some
suitable parts on selected levels to controlling all parts on all levels.
Summary
In fact, many world-changing engineering applications of AI involve more than one
capability of AI tools. Taking autonomous driving, for example, the autonomous
processing of image data for road condition detection, road planning via route
discovery and optimization, autonomous control of the mechanical components,
power management, object detection, and emergency judgment and treatment all
take significant roles. Such engineering applications may further expand and deepen
their influences via the incorporation of big data and convergence with the Internet
of Things (IoT).

1.3 Basics of AI
The representation, optimization, and evaluation, as well as underlying math,
constitute the basics of AI. In this section, the basic concepts will be introduced first.
What follows will be a quick overview of the common machine learning algorithms.
The challenges and issues in machine learning will be discussed next. The math
knowledge needed for machine learning, e.g., more details about optimization and
evaluation, will be presented in the appendices.

1.3.1 Basic Concepts
This subsection covers the basic concepts including key machine learning elements,
data format, and typical machine learning workflows.
Key Machine Learning Elements
Machine learning, or most machine learning algorithms, can be conceptually
divided into three main elements:

(1) Representation: What does the model look like? ↔ How is knowledge repre-
sented?

(2) Solution (optimization): How are (optimal) models generated? ↔ How is
knowledge extracted?
(3) Evaluation: How is the performance of models evaluated? ↔ How is the
obtained knowledge measured?


There are eight key concepts in the above three main elements.
Representation
Concept 1: Data. Data is the food or nutrients for machine learning and the
generation of machine learning models. Thus, data is where the experience or
knowledge is embedded. In supervised learning, data is divided into input and output
to represent the unlabeled data and labels, respectively. In unsupervised machine,
there is only unlabeled data that can be processed to find specific patterns in such
data. In reinforcement learning, data is generated as the learning agent interacts with
the environment and “labeled’ using a reward function. More explanation about the
format/structure of data will be provided in a later subsection.
Concept 1.1: Input. Input is what we feed into machine learning models for learning.
Such input data represents the observations or measurements excluding the labels
or target values. Each observation is a sample or an instance composed of values for
different aspects of the observation, which are called attributes, features, (random)
variables, independent variables, and predictors (predictor variables) in different
literature.
Concept 1.2: Output. This is what we want the models to predict or estimate. Output
is also called labels, targets, dependent variables, and response variables in different
places.
Concept 2: Algorithms. Algorithms or learning algorithms are different ways
of constructing machine learning models based on data. We can understand an
algorithm as a procedure that may contain one way of conceptualizing the model,
mathematical equations, and logic statements. Algorithms usually can be outlined
using pseudo-code to illustrate how to implement the procedure. We can understand
algorithms as methods in a more general sense. Different algorithms can be used
to address different machine learning tasks, e.g., regression, classification, and
clustering.
Concept 3: Model. A model or a machine learning model is a math function or a
more complicated entity that can be mathematically formulated. As the product of
running an algorithm on data, a model can be a fitting function in the simplest case
or neural networks with fixed weights. We may encounter untrained models, e.g.,
an artificial neural network with randomly generated weights, which have not gone
through the training process and thus contain model parameters that are irrelevant to
data. By contrast, trained or pre-trained models contain parameters that have been
determined with data and thus represent some knowledge from such data.

Concept 4: Parameters. Parameters are also called coefficients and weights, depend-
ing on the algorithms and contexts. Models can be understood to be formed by

two parts: a general template (or architecture) and detailed parameters that fix the
template into a specific object. The number of parameters can range from a few,
e.g., 2 in a linear model, and to millions, e.g., 138 million in the VGG12 deep neural
network.

Concept 5: Hyperparameters. Hyperparameters are distinct from the (model) param-
eters in that they are not part of the model. Instead, hyperparameters are those

numbers that we set in the initial configuration or setting before training machine

learning models. Such numbers are needed to determine how the learning will be
performed and mostly determined by the selected algorithm including the solver.
Typical hyperparameters include the coefficients determining the loss function,

solvers, visualization, and the way that the data and model are processed. Hyper-
parameters are critical to the success of learning tasks in addition to the selected

algorithm, model architecture, and initial model parameters (if any).
Solution
Solution is the search for the model that best extracts the knowledge from the data
with the selected machine learning algorithm. We can perform the solution by both
deriving an analytical solution, i.e., equation(s) for calculating the model parameters
directly, and using an optimizer or a solver to help us find a model. The former can
be done within one or a couple of steps, while the latter will involve an iterative
optimization process. Also, the former usually can secure the exact or best model,
while the latter, in most cases, can only help us find a local optimum as a relatively
good (approximate) model instead of the best model. While both approaches can
be adopted for many machine learning models, the former is mostly adopted for
algorithms with simple models like linear models, whereas the latter is usually
adopted for algorithms with complicated models like deep neural networks.

Concept 6: Loss/cost/objective function. Loss functions, cost functions, and objec-
tive functions are usually used interchangeably, though the objective function can

be minimized or maximized while the loss/cost functions need to be minimized.
Such a function is an essential part of the algorithm especially when approximate
optimization is needed for solution. This is because it measures the performance of
machine learning models, which can determine the optimization direction during
the solution process. The idea is to minimize the loss function so that the most
appropriate parameters for the machine learning model can be obtained.
Concept 7: Optimization methods (solvers and optimizer). Optimization methods,
or called solvers and optimizers, dictate how the solution process is performed
to optimize the loss function. The optimization method is usually not fixed
when developing an algorithm, and it entails knowledge that is much different
from and independent of the machine learning algorithms and models. Therefore,
optimization methods can be separate from the algorithms. Common solvers are
borrowed from the optimization realm. Such methods can work as long as a loss
function, parameters to be optimized (e.g., model parameters in machine learning),
and constraints (if needed) are given.
Evaluation
Evaluation is usually implemented during testing or cross-validation to assess the
performance of the model. Evaluation needs to be discussed with respect to different
machine learning tasks (or problems). This is because different evaluation metrics
have been proposed for different types of tasks, e.g., classification, regression, and
clustering. A key in machine learning model evaluation is the selection of the
evaluation metrics.


Concept 8: Evaluation metrics present different methods for measuring the perfor-
mance of a model including but way beyond accuracy and error. A complete list and

description of the common metrics will be provided in the appendices.
Data Format
This subsection discusses how to organize data, or, in other words, the structure of
data. However, the term “data structure” has a connotation of the way of organizing
and storing data in a computer’s memory or storage. Thus, we here use data format in
the title to avoid confusion. Despite this arrangement, data format and data structure
are interchangeable in this book, and both refer to the structure or organization of
data on a higher level that human beings can easily understand or visualize.
Data is a collection of numbers and symbols that we use to describe things.
In machine learning, data is usually used to quantitatively describe measurements
(or called observations) and their assessments. Therefore, an intuitive structure of
data is to organize different measurements as different data points, which are the
elements in the data structure. Due to the same reason, a data point can also be
called a sample, an instance, or an observation (a record occasionally). As shown
in Fig. 1.6, every observation may contain information for multiple aspects; thus,
every data point may also have different values termed as attributes or features (less
frequently as property). Data points are usually grouped together as a dataset for a
purpose, e.g., training and testing a model, leading to training and testing datasets,
respectively. Usually, all the data points in the same dataset have the same number
of attribute values. The assessments of data are stored as labels (or called targets)
in the dataset. In a labeled dataset, each data point has one to multiple label values.
Every data point with its label(s) is called a labeled sample, a labeled instance, or
an example.


The realizations of data structure for different machine learning purposes share
a lot in common in spite of minor differences between such realizations in different
packages. Figure 1.6 presents a representative way of organizing data. One item
that was not mentioned above is the sample ID. In some AI tools, the sample ID is
explicitly used for referring to different data points, while in some other tools, ID is
not explicitly defined, and parameters such as the location, row number, and order
are implicitly used as the ID. As can be seen, it is common to use different rows for
different data points and different columns for different attributes. Label values are
usually stored either as a separate array or column(s) to the right of the unlabeled
data.
As can be seen in the above introduction, many terms are used interchangeably,
though there may be some negligible differences. In this book, though these terms
are used interchangeably most of the time, we try to follow the convention of
their use in individual machine learning topics. In addition to the convention,
though not always true, the following rules will be followed. “Data point” is
preferred when talking about plotting, space, and distributions; “sample” is used
when an experimental context is emphasized; “instance” is used in a pure machine
learning context. “Attribute” is preferred for samples whose individual aspect can
be described with simple math entities such as a real number; “feature” is used
for samples with more complicated characteristics, such as those that need to be
represented by a collection of numbers. “Label” is used when emphasizing the
application or experimental flavor; “target” is used in contexts with heavy math
content.
Finally, we need to point out that data has been a core component in both data
mining and machine learning. However, the role of data can still be slightly different
due to the different objectives of these two areas: data mining aims to uncover
patterns in the data, while machine learning is intended for reproducing known
patterns and making predictions based on them. That is, data is being explored in
data mining to gain knowledge; by contrast, data is used in machine learning for
creating models that can handle future prediction tasks.
Machine Learning Workflow
The general workflow of machine learning can be summarized as a process of
preparing data and feeding it into a model so that the model parameters can
be optimized to reach a model with satisfactory performance in the analysis
of new data. Despite the common traits, the detailed workflow of performing
machine learning varies across different categories of machine learning algorithms:
supervised, unsupervised, semi-supervised, and reinforcement machine learning.
Typical differences are listed below.
• How is data prepared? For example, collected before training (supervised,
unsupervised) versus during training (reinforcement learning)


• What does the data look like? For example, labeled (supervised) versus unlabeled
(unsupervised)
• How is the best performance defined? For example, labels (supervised) versus
data characteristics (unsupervised) versus rewards (reinforcement)
In fact, the process can be slightly different even between algorithms in the same
category.
Despite the differences, some terms referring to specific stages of the workflow
are widely used everywhere. In particular, the process and some terms associated
with supervised learning are more familiar to us due to the predominant role of
supervised learning in traditional machine learning. Here, we will introduce the
machine learning process in common supervised learning tasks. The workflows of
other categories of machine learning algorithms can be understood with deviations
from this one. Those workflows, together with the deviations, can be seen in later
chapters for other categories of machine learning algorithms.
Let us first take a look at how (supervised) machine learning is adopted for
addressing typical engineering problems. As shown in Fig. 1.7, the following steps
are what we typically adopt in supervised machine learning:


(1) Collect and prepare data (including data augmentation, labeling the data,
dividing data into training and testing datasets).
(2) Choose or develop a machine learning algorithm.
(3) Train the model with the algorithm using the training data.
(4) Evaluate the model using the testing data.
(5) Fine-tune hyperparameters to improve the model.
(6) Make predictions for new data samples.
A few things in the above procedure need to be clarified and emphasized.
First, data has an essential role in the above process. We can even understand this
process as a sequence of data operations.
Second, training and testing appear to be the core stages, whose significance
usually needs no emphasis. However, in real engineering practice, it is common
to find that data preparation takes a major portion or even the majority of effort.
In particular, the availability of out-of-box AI tools eases the selection and use of
algorithms/models, while data in specific engineering applications usually needs
to be collected, cleaned, labeled, and preprocessed before use. Work needed for
generating, cleaning, and augmenting data varies case by case, depending on the
data quality, algorithms, expectations for performance, etc. In many cases, data
labeling needs to be done manually, which can be trivial and labor-intensive.
Sometimes, model evaluation and fine-tuning can also take a lot of time and care.
Third, training and testing stages should use two datasets that are independent
and identically distributed (IID), which is critical to the success of the above
process. Independence and distribution are two major characteristics of datasets.

The independence implies that the two datasets share no common samples. Oth-
erwise, testing will be redundant with training in some way, depending on the

overlap. The identical distribution denotes that the random variables (attributes of
the data samples) should have the same distributions in both datasets. Otherwise,
the two datasets will have different natures, leading to two different problems for
training and testing. Particularly, a model trained to work well for one problem
very likely works poorly for another problem with a totally different nature. In

some cases, we also have a substage called cross-validation in training. This cross-
validation is different from testing in that testing usually involves exclusively IID

data, while cross-validation shares data with training. Cross-validation typically
involves shuffling and sampling steps and runs alternately with the training process.
Finally, the above process may be simpler than many actual machine learning
jobs. For example, in more advanced data analytics work, feature engineering and
feature extraction may need to be separated from data preparation as a significant,
separate step. Also, for people who are more focused on studying instead of
applying algorithms, the algorithm development and model evaluation will be
emphasized. In more complex industrial AI applications, we may also need to
involve more steps for model testing and deployment, for example, deploying the
model in a hyper care model before it goes live, during which model evaluation may
happen multiple times.


1.3.2 Common Algorithms
Overview and Machine Learning Tasks
This subsection presents a very quick overview of common machine learning

algorithms in the order of major categories: supervised, unsupervised, semi-
supervised, and reinforcement learning. However, prior to the introduction to

detailed algorithms, it is necessary to differentiate the types of algorithms from the
types of tasks these algorithms can be applied to. This is because, when selecting
algorithms, both pros and cons of algorithms and the jobs that they can do are what
we need to first consider. Unfortunately, the algorithms and the tasks that they can
handle have complicated relationships. For example, some algorithms may handle
a task that most other algorithms in the same category cannot. Moreover, such
relationships are not fixed, as researchers may extend an algorithm to perform tasks
that it could not. This is possibly one reason why such relationships are usually not
explained adequately in typical machine learning literature.
Here, we try to lay down the relationships in a simple and general way and leave
out the exceptions. The major categories of machine learning are shown in Fig. 1.8.
As can be seen, regression tasks and classification tasks are usually discussed in
supervised learning. This is because these two tasks both require labeled data:
regression requires a continuous number or an array of continuous numbers as
the label for each sample, while classification needs a discrete number (or another
symbol) or an array of discrete numbers (other symbols) as the label. Unsupervised
learning does not contain labels, so it is usually used to process data by analyzing the
relationships between data points, such as distances between points. Typical tasks

for unsupervised machine include clustering, dimension reduction, anomaly detec-
tion, and association rule learning. Reinforcement learning is more for planning and

decision-making. It helps the learning agent improve its decision-making ability by
letting the agent interact with an environment. Therefore, reinforcement learning
is also adopted for “learning”: helping models created by other algorithms gain a
higher learning ability.
In a more abstract way, it can be interpreted as that machine learning can be
used for predictive, descriptive, and prescriptive tasks. The predictive function helps


predict what will happen with data. The descriptive function helps explain what
has happened with data and what the data conveys. The prescriptive function helps
make suggestions about what action to take based on data. These three functions
correspond to the above three categories to some extent but not completely.
An algorithm from any of the three categories may provide multiple functions
depending on how the algorithm is applied.
Figure 1.9 lists the machine learning topics (or types of algorithms) and the
popular algorithms in each topic within the three major machine learning categories
as well as the background knowledge. The listed topics and algorithms are selected
based on a pragmatic criterion: what are the most needed and valuable contents for
a typical semester-long course in college? Accordingly, such topics correspond to
sixteen weeks, in which some topics are marked with * to indicate that they can be
optional if the semester is shorter. In the following, common algorithm types and
representative algorithms for each type will be explained using language with as
little technical detail as possible to help everybody get a quick understanding of the
ideas behind the algorithms.
Supervised Learning
Linear Models refer to a category of algorithms that can be used for regression
analysis or curve fitting, which engineers are familiar with. Thus, we can simply
understand this as the use of a simple mathematical function to fit the measurement
data, in which attributes are the independent variables (xs) and the label (if only
one label) is the dependent variables (y). In a narrow sense, such a model adopts
a linear function: y is a linear function of xs (or linear combination of xs). In the


simplest case, a linear function parameterized by the weights for different dependent
variables and the bias, which can be directly obtained via an analytical solution,
forms the basic linear model. To overcome overfitting issues, L2, L1, and the
combination of L2 and L1 norms of the model parameters can be considered when
searching for the best curve fitting function, leading to Ridge, Lasso, and elastic net
algorithm, respectively. These linear models are only suitable for linear regression
problems. In a broader sense, these linear models can be extended with kernel
functions to deal with nonlinear problems. Linear models are initially proposed for
regression or curve fitting problems, so they are of a numeric nature—both attribute
and label values are numeric. Notwithstanding, we can make further changes like
adding a logistic function or a Softmax function to convert the output of linear
models to a probability and finally to a discrete label. In this way, linear models
can also be applied to classification problems.
Decision Trees are a family of algorithms that use a treelike structure to
guide/mimic humans’ decision-making process. Starting from a node representing
the root of the tree, each node, including the root node and any node on the
following layers except the leaf nodes, is associated with an attribute. The possible
values of the attribute will lead to the generation of “son” nodes on the next layer
(or splitting). Thus, in a decision-making process like classifying a sample, the
sample will start from the root node. Then, depending on the attribute values, the
sample will move from one node to another one on the next layer based on the
value of the attribute associated with the node and eventually to a leaf node. Each
leaf node is associated with a class. This is how a classification decision is used
for prediction. For training, all labeled samples will start from the root node as
well. Then, an attribute is selected at each non-leaf node according to a general
criterion of reducing the chaos (uncertainty) in the data the best. The node-by-node
selection of attributes and concurrent splitting of the dataset into subsets belonging
to nodes on the next level continue until we reach a node where further splitting
is not possible, e.g., reaching a leaf node. This process will generate a decision
tree according to this dataset. Different criteria for selecting the attributes lead to
different decision tree algorithms: ID3, C4.5, and CART. Pre- and post-pruning
techniques are very important in controlling the tree size, e.g., depth, to deal with
overfitting. These techniques are also frequently included as part of the modern
decision tree algorithms.
Support Vector Machines, or more commonly referred to as SVMs, are a machine
learning topic that once held a predominant role before deep learning rose. Such
machine learning algorithms stem from the idea of performing binary classification
by finding the widest margin to separate the samples or the corresponding data
points from the two classes. This conceptualization leads to a typical constrained
optimization problem: maximize the width of the margin by searching for the
margin boundaries controlled by both the margin direction and the locations of
samples on the boundaries, which are the “support vectors.” Accordingly, no
samples can get into the margin or the region belonging to the other class. This
strict constraint called “hard margin” can be loosened as “soft margin” to allow for


noisy data points. This unconstrained optimization problem, though can be solved
using an optimization package, is more commonly converted to a dual problem
to facilitate a more convenient computer solution to the corresponding quadratic
convex optimization problem. Besides, the basic linear SVM can be extended with
kernels to address nonlinear problems. The basic version of SVM was proposed
for binary classification tasks. However, it is not difficult to extend it to multiclass
classification and regression tasks.

Bayesian Methods are also called Bayesian algorithms, Bayesian machine learn-
ing, and probabilistic machine learning in the literature, though the exact meanings

of these terms may slightly differ depending on the context and research areas.
Bayesian methods were intrinsically constructed for classification tasks. Possibly
due to this reason, the use of the “Bayesian classifier” is very common or even
predominant in some technical publications. Such machine learning algorithms
stem from Bayesianism, which uses probabilities to quantify the level of belief and
consequently updates such beliefs in the evidence of new data. Accordingly, such
methods correlate the probability of a sample with a certain combination of attribute
values and target values with the probabilities that the different classes and attribute
values appear conditionally or simultaneously. Depending on how to calculate
such probabilities, we have different types of Bayesian algorithms: from naive
Bayes, which assumes strong (naive) independence between attributes, to Bayesian
network, which can formulate complicated interdependence between different
attributes with a network-like probabilistic graphical model. It is worthwhile to
mention that such Bayesian classifiers, which belong to parametric machine learning
methods, can also be extended to handle regression tasks. Other nonparametric
Bayesian methods like Gaussian process are proposed more for regression and may
appear much different from traditional Bayesian classifiers.
Artificial Neural Networks (ANNs or simply called NN in machine learning) use
mathematical operations like inner products, element-wise products, and activation
functions that can work on arrays to mimic the working mechanisms of biological
neural networks. In particular, most modern ANNs adopt the M-P neuron model:
the input to a neuron is multiplied by the neuron’s weights and then the difference
between this product and a threshold is fed into an activation function to generate the
neuron output. A network that typically consists of multiple layers of such neurons
treats and processes the data according to the architecture of the network, e.g.,
direction and interneuron/interlayer connections, and finally outputs the predicted
label. One key piece of knowledge in ANN studies is how to train an ANN. So
far, backpropagation has been accepted as the most common method for training
ANNs, in both shallow ANNs and deep learning. A typical backpropagation process
includes a forward pass for predicting the label and a backward pass for passing the
“gradients” of loss to different model parameters, e.g., network weights, to update
these parameters so that the ANN can make better predictions in the next forward
pass. Multilayer feed-forward neural network, especially 3-layer, is the most widely
known ANN architecture, though other types, such as RBF, are also available.


Deep Learning is about the development and use of deep NNs. The “deep” in this
definition, in general, denotes the depth of layers in an ANN. A neural network
that consists of more than three layers—which would be inclusive of the input and
the output layers—can be considered a deep learning model. Thus, in theory, it
can be viewed as a subset of or an extension to traditional ANNs. However, the
breakthroughs and widespread applications of deep neural network have gained
knowledge that makes deep learning much different from traditional (shallow)
ANN studies. Some of the typical factors that contribute to the development of
ANNs especially the transition from shallow NNs to deep learning, including
breakthroughs specific to deep learning and a few advances from the general field
of machine learning such as pre-training, transfer learning, solvers, and regularizers
are listed as follows:
• Better data: more data, preprocessing, normalization
• Better weights: initialization, pre-training, transfer learning
• Better network structure: activation, batch normalization, CNN, LSTM, NIN,
residual network, transformer
• Better solvers
• Better regularizers
• Better computing resources: GPU, parallel computing
Deep learning in the third wave of AI exhibits its success via convolutional neural
network (CNN) in computer vision, recurrent neural network (like long short-term
memory) in natural language processing, integration with reinforcement learning
for learning and control, and more recently in large language models (LLMs).
Ensemble Learning refers to algorithms that employ models generated by other
algorithms as constituent models to obtain an ensemble model with performance
that is better than what can be obtained with individual constituent models. These
constituent models, which are called base models, can be generated by one of those
basic machine learning algorithms, like linear model, SVM, decision tree, KNN and
NN, or a combination of them. The former is called homogeneous, while the latter
is heterogeneous. Currently, homogeneous base learners are more frequently used,
and among them, CART decision tree and neural networks are the most common
algorithms for generating homogeneous base learners. The idea behind ensemble
learning can be described as “union is strength” or “many hands provide great
strength.” Therefore, ensemble learning is viewed as an optimization method that
generates a strong learner from several weak learners. There are three common
categories of ensemble learning methods: bagging (e.g., meta-estimator [basic
bagging] and random forest), boosting (e.g., AdaBoost and Gradient Boosting), and
stacking. Bagging is focused on the utilization of democracy to reduce variance.
Boosting, which features elitism, generates better or elite models by focusing on
samples associated with wrong predictions and gives elite models more weights
in decision to improve bias. Stacking replaces simple combination rules with a
machine learning model as a second layer learner to process the results from the
first layer learners for better predictions.


Unsupervised Learning
Clustering is a machine learning topic aiming to divide unlabeled data (data points
or samples) into different groups. A general goal is to generate groups so that
samples within the same group are similar to others while samples from different
groups are different from each other. These groups or groupings are referred to
as “clusters.” Both the ways of generating clusters and evaluating the generated
groups (or clustering process) are the content of this unsupervised learning topic.
For the former, many different types of clustering algorithms have been proposed
based on the patterns that the data points need to be arranged in. Centroid models
refer to clustering algorithms wherein the clusters are formed by the proximity of
the data points to the cluster center or centroid. Data points are clustered based
on multiple centroids in the data. K-Means clustering and mean-shift clustering
are the most popular algorithms in this category. Density models are generated
by clustering algorithms that group data by areas with a high concentration of
data points surrounded by areas with a low concentration of data points. DBScan
and OPTICS are two popular density-based algorithms. In distribution models,
data points are grouped together based on the probability that they may belong
to the same distribution. In a particular distribution, the distance of a data point
from a center point is determined to infer the probability of being in that cluster.
Gaussian mixture theory (GMM) is the most common option in this category.
Hierarchical (connectivity) models involve top-to-bottom or bottom-up hierarchies.
Agglomerative hierarchical algorithm is the most popular example in this category.
Dimensionality Reduction, or called dimension reduction, is the transformation
of data from a high-dimensional space, e.g., with a large number of attributes, into
a relatively low-dimensional space, e.g., with fewer attributes than the original data,
while retaining essential properties of the original data. Therefore, dimensionality
reduction helps remove redundant or less significant variables. These methods can
be classified into two major categories: feature selection and feature projection.
Feature selection seeks to find a subset of the input variables (or features, attributes,
dimensions). Therefore, we select features directly according to some criteria, such
as filter strategy (e.g., information gain) and wrapper strategy (e.g., search guided
by accuracy), and drop the less desired features to reduce data dimensionality. By
contrast, feature extraction tends to project the data in a high-dimensional space to a
space of fewer dimensions, in which the more desired features or their combinations
are extracted. Feature selection methods primarily refer to those data reduction

techniques studied in statistics for variable selection and now mostly in high-
dimensional regression analysis. Classical methods are missing value ratio, low

variance filter, high correlation filter, random forest, backward feature extraction,
and forward feature selection. Feature projection methods are more popular in the
dimensionality reduction literature and are even used to represent dimensionality
reduction in a narrow sense. This category includes the most popular dimensionality

reduction methods such as principal component(s) analysis (PCA), linear discrim-
inant analysis (LDA), independent component analysis (ICA), Isomap, and MDR.


Dimensionality reduction methods can also be classified based on whether they are
for labeled or unlabeled data. A typical example is that PCA was developed for
unlabeled data. Thus, PCA is mostly used for clustering in unsupervised learning.
On the contrary, LDA was proposed for processing labeled data. Another way of
classifying dimensionality reduction methods is based on whether these methods
are linear or nonlinear in nature. For example, PCA, ICA, and LDA are linear, while
LLE, Isomap, MDR, and kernel PCA are nonlinear.

Anomaly Detection, also called novelty detection, outlier detection, forgery detec-
tion, or out-of-distribution detection in different areas, is intended to identify rare

items, events, or observations that significantly deviate from the majority of the
data and do not conform to a well-defined notion of normal behavior. Anomaly
detection has been applied to a variety of areas such as fraud detection, web hack
detection, medical (disease) detection, sensor network anomaly detection, IoT bid
data anomaly detection, log anomaly detection, and industrial hazard detection.
In a broad sense, the available methods for anomaly detection can be roughly

grouped into rule-based methods, statistics-based methods, and machine learning-
based methods. Among them, the anomaly detection methods based on machine

learning algorithms are anomaly detection in a narrow sense and represent the state
of the art. The machine learning-based methods can be further categorized into
supervised, unsupervised, and semi-supervised methods. In unsupervised learning,
common methods can be divided into five groups: statistics-based, distance-based,

density-based, clustering-based, and tree-based. In semi-supervised learning, pop-
ular methods include one-class SVM, AutoEncoder (or autoencoder), and GMM.

In supervised meaning, we usually need to pay attention to data labeling and
imbalanced data for possible issues, and such methods are suitable for considering
data with new classes. Common methods in this category include linear models,
SVM, and ANN.
Association Rule Learning, which is also called association rule analysis and
association rule mining in many publications, is a rule-based type of machine
learning for discovering interesting relations between variables. This definition is
not that straightforward to understand, especially considering the vague meanings of
terms like “relations between variables,” “rule,” and “interesting.” Also, association
rule learning and algorithms (or methods) for it contain various new parameters/-
concepts, such as those embedded in the definition. This fact makes it difficult for
people without some expertise in data mining to understand the topic and implement
association rule learning algorithms. In addition to these parameters and concepts,
association rule learning treats data that are usually formulated in a format slightly
different from data dealt with in other machine learning areas due to historical and
practical reasons. Therefore, association rule learning may appear much different
from other supervised and unsupervised machine learning topics in many aspects,
which can further confuse learners. Popular association rule learning algorithms
include Apriori, FP growth, and Eclat.


Reinforcement Learning
Reinforcement Learning is the third category of machine learning, in which no
raw data is given as input. Instead, the reinforcement learning algorithm needs to
design a way to generate and label data in the training process. Reinforcement
learning is frequently used for robotics, gaming, and navigation. With reinforcement
learning, the algorithm discovers through trial and error to identify actions yielding
the most significant rewards. Thus, actions and rewards are similar to the attributes
and labels in supervised machine learning in some way. As illustrated in Fig. 1.10,
this type of training has three main components: an agent that can be described
as the learner or decision maker, an environment that the agent lives in, and the
actions that the agent takes for rewards. The objective is to let the agent take
actions that maximize the expected reward over a given measure of time. The
agent will reach the goal much quicker by following a good policy. So the purpose
of reinforcement learning is to learn the best policy. Reinforcement learning can
be roughly categorized into value-based, policy-based, and hybrid (involving both
value and policy) algorithms.

Typical examples of value-based reinforcement learning algorithms are Q-
learning and Sarsa. In Q-learning, the agent learns by updating its own Q table,

which quantifies the values of different actions in specific states. The agent interacts
with the environment and generates a sequence of state-action-reward values called
a trajectory. The yielded rewards will help the agent update its own Q table and
consequently improve the decision-making ability. Q-Learning is an off-policy
method because it learns an optimal policy no matter which strategy it carries
out. Sarsa follows a very similar procedure for learning, but it adopts the current
policy for decision-making during the learning process (or call episode) and thus
is an on-policy algorithm. By contrast, policy-based reinforcement learning does
not use action and state values to determine the optimal action. But instead, it
adopts a probability function for selecting an action. The selected action will
be evaluated via the reward function to update the policy parameterized by this
distribution. A typical example of policy-based algorithms is policy gradient. More
complicated reinforcement learning integrates both the values and policy such as
critic-actor and later variations like C3A. Such hybrid algorithms use one type of


reinforcement learning to generate an action and use the other type of algorithm to
assess the algorithm. In addition, reinforcement learning has been integrated with
deep learning. For example, deep neural networks can be used to replace the Q table
as the mapping from actions to values, leading to algorithms like deep Q-learning.
Many others are being proposed following this direction like deep deterministic
policy gradient (DDPG).
Semi-supervised Learning
As the fourth type of machine learning, semi-supervised learning uses both labeled
data and unlabeled data. Though semi-supervised learning can be viewed as a
hybrid of supervised and unsupervised machine learning, it is mostly used for the
same purposes as supervised learning, e.g., classification, regression, and prediction.
Semi-supervised learning usually employs a small amount of labeled data with a
large amount of unlabeled data. Semi-supervised learning brought obvious benefits
such as the save in the effort of labeling massive amounts of data and the reduction
in the bias caused by labeling. Such algorithms still need to pose strict requirements
on the data, such as the accuracy of the labels for labeled data and class balance
for the unlabeled data. Semi-supervised learning has sub-categories like simple
self-training, co-training, label propagation algorithm, semi-supervised SVM, and,

more recently, semi-supervised deep learning. The workflows of different semi-
supervised learning algorithms may be considerably different. Figure 1.11 illustrates

a typical workflow in self-training.


Summary
AI is a highly dynamic field. In the third wave of AI represented by machine learning
algorithms, the popularity of the algorithms is changing every day. Figure 1.12
presents is a list of common algorithms (top-10 ranking) according to a survey
conducted by Kaggle with 18,996 respondents. This presents a snapshot of this field
for a condition around 2020.

1.3.3 Challenges and Issues in Machine Learning
Data Issues
As the “food” for machine learning, data, especially its health, can easily affect or
even determine the development, validation, and application of machine learning
models. Unfortunately, real-world problems usually do not provide datasets that
have been well assessed, structured, and annotated as those well-documented
datasets from machine learning packages, such as the Iris dataset from Scikit-learn.
In engineering, we usually need to use data collected in different environments
in different ways, which may lead to highly heterogeneous, highly unstructured,
incomplete, erroneous, and unknown data. Such data may consume a lot or even
most of the project time and threaten the validity of machine learning models built
on it. In short, common data issues can be summarized as inadequate data, immature
data, incorrect data, noisy data, and biased data.


Inadequate data is a very common issue in traditional machine learning, espe-
cially before the advent of big data. In many cases, a major cause of poor

performance of machine learning models is an inadequate amount of data. That
is why much effort in traditional machine learning was devoted to the utilization
of data such as sampling techniques for cross-validation like bagging. Even after
we entered the era of big data, inadequate data is still haunting over many machine
learning tasks. This is because, as the complexity/capacity of a model increases,
the amount of data that is needed for the model to reach the same performance also
increases. Besides, the inadequacy of data sometimes does not necessarily mean that
the number of samples is not enough, but instead, the number of usable samples is
not adequate. For example, it is usually not difficult to obtain images for computer
vision; however, labeled images, which may need labor- and expertise-intensive
work to generate, can be hard to obtain. Data augmentation, innovative labeling
techniques, and semi-supervised machine learning are common solutions to data
inadequacy.
Immature data refers to data that will need a significant amount of preprocessing
work before it can be used for training or testing a model. This is very common
as data may be incomplete, heterogeneous, and structured in ways that are not
compatible with a model. For example, many deep learning models will require

a specific shape of the input arrays and a certain way of labeling the data (e.g., one-
hot labeling). This usually requires us to spend a lot of time converting every sample

and label into the required format. The workload can be astonishing when we need
to deal with a great amount of such data. Under this condition, it could be even more
difficult to tell what data immaturity issues the data may have, because it is hard to
check the samples one by one as it may take minutes or hours to load the dataset. It
will be helpful to develop code that can automatically assess and preprocess the data,
such as format check, trimming, resizing, and removal, though the development of
such code may also be time-consuming.
Incorrect data is another type of data issue. Compared with other issues, this
type of issue is hard to detect and, if overlooked, can cause serious outcomes such
as incorrect models. One typical example is mislabeled data. For example, some
samples may be assigned wrong labels due to a variety of reasons. Unfortunately,
when such data is used for training, the misinformation will also be learned by
the model. In particular, for instance-based algorithms such as KNN, the data will
be included as part of the model. Errors will also be integrated into such models,
leading to problematic predictions for future data. In regression tasks, wrong label
values due to systematic errors such as sensor drift are also this type. Manual
data assessment by human experts and algorithms that can detect such issues. For
example, anomaly detection algorithms can be used to identify these issues.
Noisy data is very similar to incorrect data but can be different in some ways.
It is characterized by the existence of a small amount of data that exhibits trends
different from the others. So, it can be caused by random errors like those mislabels
due to accidental operations, which affect specific data points, rather than systematic
errors, which can lead to offset in all the data. Furthermore, it is also possible that
the noise appearance is not caused by an error, but instead, due to the distributions


associated with the data. Such issues can be handled by both processing the data
to screen out the noisy samples and developing more rigorous algorithms. For
example, the soft margin in the SVM can help consider samples that do not meet
the basic assumptions of data distributions.
Biased data is produced when certain samples are heavily weighted or need
more importance than others. Such data causes a typical issue: the data cannot
represent the real problem or cannot be representative of new cases that we need to
generalize. For example, a training dataset does not cover all cases that have already
occurred and/or are occurring. Biased data may lead to inaccurate predictions,
skewed outcomes, and other analytical errors. In other words, the model may learn
from data that only represents a part or an aspect of the problem and extend the
knowledge to the whole problem. Such issues can be resolved by determining where
data is actually biased in the dataset and countermeasures can be proposed to rectify
the bias.

Inductive Bias
Strictly speaking, inductive bias is not an issue. However, it can cause issues if we
do not understand it and treat it properly. It is among the concepts that are the most
difficult to understand in machine learning. Meanwhile, it is an essential element
of machine learning, though it may not even be noticed by many machine learning
practitioners. However, a better understanding of it can help us search for more
suitable models and avoid issues due to inappropriate selection or treatment of the
inductive bias.
Inductive bias can be formally defined as the assumption(s) that a machine
learning algorithm adopts to generalize a limited set of observations (training
samples) into a general model. As introduced in the section for symbolic versus
numerical AI, machine learning as a numerical AI boils down to induction: the
process of moving from special observations to general rules or models.
Inductive bias is needed because we will need to provide information to describe
what is “general” in induction. Take the regression problem in Fig. 1.13 as an
example. Two models, i.e., Model 1 (linear, green) and Model 2 (nonlinear, blue),
can be obtained based on the same training dataset (black circles). These two models
exhibit the same performance if we use typical regression metrics like mean absolute
error because both curves pass the centers of all the training data points. In this case,
how can we tell which model is better or reflect more general rules?
We can rephrase the above problem using machine learning terms to obtain a
strict description. First, supervised learning can be viewed as a process of searching
in a set of all possible mappings, or, more broadly, hypotheses. This set is called
the hypothesis space. The learning goal is to find a hypothesis that can match or
provide the best description of the training data. However, in many cases, there
is more than one hypothesis from the hypothesis space that is compatible with
training data. These compatible hypotheses constitute the version space. Just like
the above example, both Model 1 and Model 2 are the best fitting models. In this

case, if no information about future data is provided, we will need a bias to indicate
our inclination or preference for selecting a model. “Occam’s razor” principle is
a common inductive bias. This principle states that we should choose the simpler
model when two models exhibit comparable performance.
The word “bias” already indicates that inductive biases present priori and
subjective information. So, it does not mean that they are always correct. Getting
back to the above example, we can see that if data that will appear in future
applications for the model is more like the blue dots, then the nonlinear model is
better. On the contrary, if the future data is more like the green squares, the linear
model is better. This leads to an extremely important idea in machine learning:
it makes little sense to talk about models without mentioning the data. Thus,
instead of saying a model is good or not, we may need to say whether it is suitable
for an application or the data associated with that application. When the target
application changes, or more essentially, the probability distribution of the possible
data associated with the application changes, we may also need to adjust the model
so that it can maintain its performance.
We can see from the above example that it will be helpful to get a better
understanding of possible future data to avoid issues that may be caused by inductive
bias. That is also the reason for introducing testing data. However, even if we
do that, the use of inductive bias is also inevitable in many cases. An extreme
example is illustrated in Fig. 1.14. In this example, the two models have the same
level of complexity and performance; thus, we will need another hypothesis on top
of Occam’s razor rule. Though this example is too simple and special, it shows
the idea that we may need to use multiple hypotheses on different levels to help
determine which model is better without knowing anything about the data that we
will encounter.
Inductive bias is hard to understand also because it may appear in different
formats in different algorithms. In the above regression problem, Occam’s razor
is widely accepted as an inductive bias in regression and has been incorporated


in many regression algorithms via regularization terms. However, it is neither the
only option nor a must-have inductive bias. For example, a unique inductive bias—
selecting the SVM with the widest margin—is explicitly specified in most SVM
algorithms. Extra examples of inductive biases include the maximum conditional
independence in naive Bayes classier, minimum number of features in feature
selection, and nearest neighbor in KNN.
It is worthwhile to mention that inductive bias not only determines which solution
will be selected, but also affects whether we can efficiently find a solution. From
this perspective, we can also understand it as constraints that we place there, which
may affect both the solution result and solution process. Let us take a look at deep
learning as an example. The inductive bias of CNN can be locality (elements in the
space show higher correlation as they get closer) and spatial variance (the kernel
weights are shared). The inductive bias of RNN is sequentiality (points that are
close to each other in time are related) and time invariance (RNN share weight
cross time steps). Thus, these two types of deep NNs can be viewed as special cases
of fully connected deep NNs, which assume all elements can be related. The extra
inductive biases help CNN and RNN search for solutions along the biased directions
for computer vision and natural language processing problems, respectively. They
generate faster and more accurate results than those general deep NNs without such
biases, because computer vision and natural language processing problems exhibit
locality and sequentiality, respectively.
Thus, the selection of inductive bias does not only affect the usefulness of the
model, but also determines how a model can be constructed and identified. An
opinion in recent computer vision research is that traditional CNNs involve too
much inductive bias. Thus, deep learning algorithms like self-attention in the ViT
(vision transformer) can provide better functions by loosening the constraints placed
by the inductive bias. More recently, multilayer perception, which has less inductive
bias, can be used to achieve the accuracy of SOTA model in ImageNet. This leads
to a controversial conclusion: is inductive bias in CNN not needed? In fact, a better


way to understand this is that inductive bias helps reach a balance between fast and
accurate solution and flexibility. When we do not have strong power to obtain a
solution, e.g., better data, higher computing power, and more efficient algorithm, it
is better to use inductive bias to help us stay more focused so that we can find an
acceptable solution or reach it more quickly. But when our power for solution is
satisfactory, we can remove some inductive bias or loosen the constraints so that we
can find better solutions or better ways of reaching the solutions.
Underfitting and Overfitting
Generalization
As explained, machine learning represents a process to learn general rules from
specific observations. From this perspective, the goal of machine learning is to
generalize from the training data to any data from the problem domain. A good
model will allow us to make predictions for data that will appear in future
applications, which the model cannot see in the training stage. Thus, we use
the concept of “generalization” to tell how well a model trained with specific
observations can perform on the data that it will be applied to.
Overfitting and Underfitting
Two issues, or outcomes of poor generalization, of machine learning models are
overfitting and underfitting. In fact, overfitting and underfitting are also the two
major causes of poor performance of machine learning algorithms. As we showed
in the previous section, without any knowledge about the future data, we can only
rely on inductive bias to assess/select models. To improve the model selection, we
usually split the available data in supervised learning into a training dataset and a
testing dataset. In this way, we can have the testing data as a representation of future
data. The trained model can then be assessed using the testing data. Let us assume
both the training and testing data can perfectly represent all the possible data.
Then, a model that is trained with the training dataset and can achieve comparable
performance on the testing dataset is believed to have good generalization.
Figure 1.15 gives an illustration of underfitting, good fitting, and overfitting as
well as training and testing data. If a model’s performance on training data is poorer
than on testing data, e.g., lower accuracy and higher loss, we can infer the low
generalization may be caused by overfitting. If poor model performance is observed
on both datasets, then underfitting may be the reason. High bias (overall offset)
and low variance (high scatter), which will be introduced in detail in the chapter
for ensemble learning, are two common indicators of underfitting. Good fitting is
associated with good results, e.g., high accuracy and low loss, on both the training
and testing datasets.
It is worthwhile to mention that, usually, training data and testing data cannot
perfectly represent all the possible data. However, to ensure the above process for
assessing and improving generalization is valid, we will need to make sure the
training data and testing data are independent and identically distributed (written


as i.i.d. or IID). The independence requirement ensures the two datasets are not
the same thing. Otherwise, it does not make sense to separate data into these two
sets. The requirement of identical distribution helps us enforce that the training data
and testing data have the same nature or come from the same data domain. If we
consider the random variables corresponding to different attributes follow certain
distributions for the problem(s) that we want to address, then we want to make sure
such distributions in the training data, testing data, and all the possible data are the
same.
Underfitting is easier to address compared with overfitting. A straightforward
way is to increase the model complexity or switch to a new model with higher
capacity (complexity). The detailed changes will be different in different algorithms.
Taking NN for example, we can add in more layers and neurons. Sometimes, the
capacity of the model may be enough. What is truly needed is to perform more
thorough training. For example, some algorithms use iterative optimization to search
for the best model. In such a case, we will need to wait until enough iterations or
epochs are finished so that the loss can gradually decrease to an acceptable value.
If any technique for addressing overfitting is used, we may also need to reduce the
effects of such techniques to alleviate underfitting, e.g., reducing the weight of the
regularization term in the loss function.
Techniques to Prevent Overfitting

Overfitting can be addressed or controlled from three different angles: (1) control-
ling/reducing model complexity, (2) better monitoring and controlling the training

process to avoid over-training, and (3) making data better represent the whole data
domain (or sample space).
For the model complexity control, different types of machine learning algorithms
may use different techniques. But the use of regularization and model trimming
techniques is very common in most supervised learning algorithms.

Regularization constrains the model complexity by including functions of model
parameters as a penalty term in the loss function. In this way, the complexity of the
model needs to be considered when searching for the optimal model during training.
(w, b,  α)  = αw + f (err)] (1.3)
where f (err) represents the original loss as a function of error, w is the norm
of the vector (or higher order tensor) containing model parameters, and α is a
parameter that we use to adjust the significance of the complexity term in the
optimization. The higher the α value, the more likely that simpler models will be
sought. A norm is a measure of the magnitude of a vector—in this case, it tells the
size of the array composed of all the model parameters. Usually, we use L1 or L2
norms, leading to L1 and L2 regularization, respectively.

Model Trimming can be implemented in different ways depending on the algo-
rithms. For example, in decision trees, this can be done by removing part of the tree

and limiting the maximum depth (or maximum layers) of the tree in the so-called
pre-pruning or post-pruning process, respectively. In deep learning, we can add
in dropout and pooling layers to intentionally leave out and combine information,
respectively.
Overfitting can also be addressed by better understanding and controlling the
training process. The following are some widely used techniques.
Holdout is very useful for controlling overfitting by better informing the extent of
overfitting. This is actually what we did for splitting data into training and testing
sets, e.g., 60%/40% and 80%/20%. As introduced above, the difference between
the performance indicators of the two datasets serves as a measure of overfitting. If
overfitting occurs, we then should stop training and take some other actions. This
overfitting prevention technique has been widely accepted as an essential step of
training; thus, sometimes, it is not even considered a special technique for dealing
with overfitting.
Cross-Validation is very similar to testing, but it is counted as part of the training
process. Similarly, we split our dataset into k groups, i.e., k-fold cross-validation.
Then we use one group for testing and the others for training during a training
time unit, e.g., a certain number of iterations or epochs (one epoch means looping
over all the samples once). Next, another group will be used for testing, while the
remaining groups will be used for training during the next training time unit. This
process is continued until the training is finished. We can see that this can allow us
to check the extent of overfitting even before the testing. Compared with holdout,
cross-validation allows all data to be eventually used for training while being more
computationally expensive.
Early Stop is an action we usually take when we find overfitting occurs. In the
typical loss versus iterations/epochs plot during cross-validation, once the validation
loss stops decreasing but rather begins increasing and far exceeds that of the training


loss, we stop the training and save the current model. An early stopping trigger can
be set to stop the training automatically.
We can also help prevent or eliminate overfitting by better preparing the data.
One goal of data improvement is to make the data better represent the
sample space. This can be done directly by increasing the amount of data. Data
sampling techniques that can generate data to better reflect the distributions of the
real data would also help. Data augmentation is another popular technique. Data
augmentation adds noise to the data. In deep learning, this can be performed by
flipping, rotating, trimming, rescaling, or shifting the image data.
Feature Selection is a good option when we have only a limited number of training
samples while each sample contains lots of features. In this case, we can select
the most important features for training, so that the model does not need to learn
that many features, which can more easily lead to overfitting. For this purpose,
different combinations of features can be selected for training models with the best
generalization. Or alternatively, we can resort to a feature selection method and use
the feature selected by this model for training.

1.4 Practice: Gain First Experience with AI via a Machine
Learning Task
>>> More and Up-to-Date Course Materials including Practices @ AI-engineer.org
<<<
1. Get familiar with the Anaconda environment (Spyder, Python, Scikit-learn,
Matplotlib). Write code to plot Y = X2 ∗ sin(X), in which X = [0, 0.1,..., 4].
Hints:
In the process, you will import packages, try basic algebraic operations with
NumPy arrays, and plot using Matplotlib.
2. Problem-solving with Scikit-learn: classification (using the Iris dataset) and
regression (using the Diabetes dataset) tasks.
Hints:
2a. Load datasets (check https://scikit-learn.org/stable/datasets/toy_dataset.html,
link to an external site to see how to load Diabetes and Linnerud separately).
Check the structure of the data in Spyder and think about why they can be used
for the intended classification/regression task.
For example,
from sklearn.datasets import load_diabetes
X, y = load_diabetes(return_X_y=True)
2b. Select a machine learning algorithm (corresponding to a command/function in
Scikit-learning) to perform the classification/regression task.
Hints:


Check https://scikit-learn.org/stable/supervised_learning.html; link to an exter-
nal site for a list of algorithms/commands/functions. You can start with support

vector machines (SVCs or SVR in sklearn.svm).
2c. Find a way to check the results (e.g., plot, evaluate using a metric [e.g.,
score method coming with SVC when using SVC for classification], and *try
predictions with new data).




